from dotenv import load_dotenv
import streamlit as st
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI
from langchain.callbacks import get_openai_callback

import os
import tempfile
from PyPDF2 import PdfReader
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import chardet

def extract_file_content(file):
    file_extension = os.path.splitext(file.name)[1]
    content = []

    if file_extension == ".pdf":
        # æå– pdf æ–‡ä»¶å†…å®¹
        pdf_reader = PdfReader(file)
        for page in pdf_reader.pages:
            content.append(page.extract_text())

    elif file_extension == ".epub":
        # æå– epub æ–‡ä»¶å†…å®¹
        with tempfile.NamedTemporaryFile(delete=False) as temp_file:
            temp_file.write(file.read())
            temp_file_name = temp_file.name
        
        book = epub.read_epub(temp_file_name)
        os.remove(temp_file_name)
        
        for item in book.get_items():
            if item.get_type() == ebooklib.ITEM_DOCUMENT:
                soup = BeautifulSoup(item.get_content(), "html.parser")
                text = soup.get_text()
                content.append(text)

    elif file_extension == ".txt":
        # æå– txt æ–‡ä»¶å†…å®¹
        raw_content = file.read()
        
        candidate_encodings = ['utf-8', 'gbk', 'gb18030', 'big5']
        detected_encoding = None
        for encoding in candidate_encodings:
            try:
                detected_encoding = encoding
                content = raw_content.decode(encoding).splitlines()
                break
            except UnicodeDecodeError:
                continue
        
        if detected_encoding is None:
            raise ValueError("Could not detect encoding for the text file.")
            

    else:
        st.warning("Unsupported file type. Please upload a PDF, EPUB or TXT file.")
        return ""

    # å°†æ–‡æœ¬å†…å®¹è¿æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¹¶é€šè¿‡text-splittingç®—æ³•è¿›è¡Œåˆ†å—
    content_str = "\n".join(content)
    text_splitter = CharacterTextSplitter(separator="\n", chunk_size=1000, chunk_overlap=200,length_function=len)
    chunks = text_splitter.split_text(content_str)

    return chunks
      

def embedding(contents):
    # create embeddings
    embeddings = OpenAIEmbeddings()
    knowledge_base = FAISS.from_texts(contents, embeddings)
    return knowledge_base
    
def ask(knowledge_base):
    # show user input
    user_question = st.text_input("è¯·å‘æ‚¨çš„æ–‡æ¡£æé—®:")
    if user_question:
      docs = knowledge_base.similarity_search(user_question)
      
      llm = OpenAI()
      chain = load_qa_chain(llm, chain_type="stuff")
      with get_openai_callback() as cb:
        response = chain.run(input_documents=docs, question=user_question)
        print(cb)
          
      st.write(response)


def main():
    load_dotenv()
    st.set_page_config(page_title="ChatDocument")
    st.header("ChatDocument ä¸æ–‡æ¡£äº¤æµ ğŸ’¬")
    
    # upload file
    uploaded_file = st.file_uploader("ä¸Šä¼ æ–‡æ¡£", type=["pdf", "epub", "txt"])

    if uploaded_file is not None:
      # æ ¹æ®æ–‡ä»¶ç±»å‹è°ƒç”¨ä¸åŒçš„è§£æå‡½æ•°
      st.write("æ­£åœ¨æå–å†…å®¹ï¼Œè¯·ç¨ç­‰...")
      chunks = extract_file_content(uploaded_file)

      st.write("æå–çš„å†…å®¹å¦‚ä¸‹ï¼š")
      st.write(chunks)

      st.write("å‘é‡åŒ–å­˜å‚¨å†…å®¹......")
      knowledge_base = embedding(chunks)

      # ç”¨æˆ·äº¤äº’æé—®
      ask(knowledge_base)




if __name__ == '__main__':
    main()
