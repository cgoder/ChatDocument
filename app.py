from dotenv import load_dotenv
import streamlit as st
from langchain.llms import OpenAI
from langchain.chat_models import AzureChatOpenAI
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.vectorstores import Chroma
import chromadb
from langchain.chains.question_answering import load_qa_chain
from langchain.callbacks import get_openai_callback

from langchain.document_loaders import PyPDFLoader
from langchain.document_loaders import TextLoader
from langchain.document_loaders import UnstructuredEPubLoader
from langchain.document_loaders import UnstructuredPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

import os
import re
import tempfile
# from PyPDF2 import PdfReader
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup

import chardet


def replace_newlines_and_spaces(text):
    # Replace all newline characters with spaces
    text = text.replace("\n", " ")

    # Replace multiple spaces with a single space
    text = re.sub(r"\s+", " ", text)

    return text

def extract_file_content(file):
    file_extension = os.path.splitext(file.name)[1]
    documents = []

    with tempfile.NamedTemporaryFile(delete=False) as temp_file:
        temp_file.write(file.read())
        temp_file_name = temp_file.name

    if file_extension == ".pdf":
        # æå– pdf æ–‡ä»¶å†…å®¹
        # loader = UnstructuredPDFLoader(temp_file_name, mode="elements")
        # documents = loader.load()
        # text_splitter = CharacterTextSplitter(separator="\n", chunk_size=1000, chunk_overlap=200,length_function=len)
        # chunks = text_splitter.split_documents(documents)

        loader = PyPDFLoader(temp_file_name)
        documents = loader.load_and_split()

    elif file_extension == ".epub":
        # æå– epub æ–‡ä»¶å†…å®¹
        loader = UnstructuredEPubLoader(temp_file_name, mode="elements")
        documents = loader.load()
        # text_splitter = CharacterTextSplitter(separator="\n", chunk_size=1000, chunk_overlap=200,length_function=len)
        # chunks = text_splitter.split_text(documents)

        
        # book = epub.read_epub(temp_file_name)
        # for item in book.get_items():
        #     if item.get_type() == ebooklib.ITEM_DOCUMENT:
        #         soup = BeautifulSoup(item.get_content(), "html.parser")
        #         text = soup.get_text()
        #         documents.append(text)
        
        # # å°†æ–‡æœ¬å†…å®¹è¿æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¹¶é€šè¿‡text-splittingç®—æ³•è¿›è¡Œåˆ†å—
        # text_splitter = CharacterTextSplitter(separator="\n", chunk_size=1000, chunk_overlap=200,length_function=len)
        # chunks = text_splitter.split_documents(documents)

    elif file_extension == ".txt":
        # æå– txt æ–‡ä»¶å†…å®¹
        raw_content = file.read()    
        # candidate_encodings = ['utf-8', 'gbk', 'gb18030', 'big5']
        # detected_encoding = None
        # for encoding in candidate_encodings:
        #     try:
        #         detected_encoding = encoding
        #         content = raw_content.decode(encoding).splitlines()
        #         if content:
        #             break
        #     except UnicodeDecodeError:
        #         continue

        result = chardet.detect(raw_content)
        # loader = TextLoader(temp_file_name,'utf-8')
        st.write(result)
        loader = TextLoader(temp_file_name,result['encoding'])
        documents = loader.load()

    else:
        st.warning("Unsupported file type. Please upload a PDF, EPUB or TXT file.")
        return ""

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size = 1000,
        chunk_overlap  = 200,
        length_function = len,
    )
    chunks = text_splitter.split_documents(documents)

    temp_file.close()
    os.remove(temp_file_name)

    return chunks


def embedding(embeddings,contents):
    # create embeddings
    knowledge_db = FAISS.from_documents(contents, embeddings)
    return knowledge_db

def embedding_2_vectorDB(embeddings,contents):
    ABS_PATH = os.path.dirname(os.path.abspath(__file__))
    DB_DIR = os.path.join(ABS_PATH, "db")

    # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
    if not os.path.exists(DB_DIR):
        os.mkdir(DB_DIR)

    client_settings = chromadb.config.Settings(
        chroma_db_impl="duckdb+parquet",
        persist_directory=DB_DIR,
        anonymized_telemetry=False,
    )

    vectorstore = Chroma(
        collection_name="langchain_store",
        embedding_function=embeddings,
        client_settings=client_settings,
        persist_directory=DB_DIR,
    )

    # å­˜å‚¨å‘é‡è‡³æ•°æ®åº“
    vectorstore.add_documents(documents=contents, embedding=embeddings)
    vectorstore.persist()
    # print("vectorstore: \n",vectorstore)

    return vectorstore


def load_openAILLM():
    llm = OpenAI()
    return llm

def load_azureLLM():
    AZURE_DEPLOYMENT_NAME = os.environ['AZURE_DEPLOYMENT_NAME']
    AZURE_DEPLOYMENT_NAME_EMBEDDING = os.environ['AZURE_DEPLOYMENT_NAME_EMBEDDING']
    # OPENAI_NAME_GPT = os.environ['OPENAI_NAME_GPT']
    # OPENAI_NAME_EMBEDDING = os.environ['OPENAI_NAME_EMBEDDING']
    os.environ["OPENAI_API_TYPE"] = "azure"
    os.environ["OPENAI_API_KEY"] = os.environ['AZURE_OPEN_API_KEY']
    os.environ["OPENAI_API_BASE"] = os.environ['AZURE_API_BASE']
    os.environ["OPENAI_VERSION"] = "2023-03-15-preview"

    # åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹
    llm = AzureChatOpenAI(
        openai_api_type= "azure",
        openai_api_base=os.environ['AZURE_API_BASE'],
        openai_api_key=os.environ['AZURE_OPEN_API_KEY'],
        openai_api_version="2023-03-15-preview",
        deployment_name=AZURE_DEPLOYMENT_NAME,
        temperature=0,
        client=None,
        request_timeout=180)

    # print("url: ",os.environ["OPENAI_API_BASE"])
    # print("url: ",os.environ["OPENAI_API_KEY"])
    # åˆå§‹åŒ–å‘é‡æ¨¡å‹
    embeddings = OpenAIEmbeddings(deployment=AZURE_DEPLOYMENT_NAME_EMBEDDING,chunk_size=1)
    # embeddings = OpenAIEmbeddings(
    #     api_type= "azure",
    #     api_base=os.environ['AZURE_API_BASE'],
    #     api_key=os.environ['AZURE_OPEN_API_KEY'],
    #     api_version="2023-03-15-preview",
    #     model=AZURE_DEPLOYMENT_NAME_EMBEDDING,
    # )
    # text = "This is a test query."
    # query_result = embeddings.embed_query(text)
    # st.write(query_result)
    
    return llm,embeddings
    
def ask(llm,knowledge_db):
    user_question = st.text_input("è¯·å‘æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„ï¼Ÿ")
    if user_question:
      # åœ¨å‘é‡æ•°æ®åº“ä¸­æŸ¥æ‰¾ç›¸ä¼¼åº¦æœ€é«˜çš„TopNç»“æœ
      docs = knowledge_db.similarity_search(user_question)
      
      # èšåˆtopNç›¸ä¼¼åº¦çš„embddingsï¼Œå‘llmæé—®
      chain = load_qa_chain(llm, chain_type="stuff")
      with get_openai_callback() as cb:
        response = chain.run(input_documents=docs, question=user_question)
        # print(cb)
      
      ## å›æ˜¾
      st.write(response)
      st.write(cb)

def main():
    load_dotenv()
    st.set_page_config(page_title="ChatDocument")
    st.header("ChatDocument ä¸æ–‡æ¡£äº¤æµ ğŸ’¬")

    # upload file
    uploaded_file = st.file_uploader("ä¸Šä¼ æ–‡æ¡£", type=["pdf", "epub", "txt"])

    if uploaded_file is not None:
      # æ ¹æ®æ–‡ä»¶ç±»å‹è°ƒç”¨ä¸åŒçš„è§£æå‡½æ•°
      st.write("æ­£åœ¨æå–å†…å®¹ï¼Œè¯·ç¨ç­‰...")
      chunks = extract_file_content(uploaded_file)

    #   st.write("æå–çš„å†…å®¹å¦‚ä¸‹ï¼š")
    #   st.write(chunks)

      # åŠ è½½æ¨¡å‹
      llm,embeddings = load_azureLLM()

      st.write("æ­£åœ¨å‘é‡åŒ–å­˜å‚¨å†…å®¹......")
      knowledge_db = embedding_2_vectorDB(embeddings,chunks)

      # ç”¨æˆ·äº¤äº’æé—®
      ask(llm,knowledge_db)




if __name__ == '__main__':
    main()
